{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49aba755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d5f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def numLines(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "def loadData(filename, max_points):\n",
    "    file_len = numLines(filename)\n",
    "    f = open(filename, 'r')\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in tqdm(range(file_len)):\n",
    "        if i == max_points:\n",
    "            break\n",
    "        line = f.readline()\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        inputs.append(line[0])\n",
    "        outputs.append(line[1])\n",
    "    data = {'inputs': inputs, 'outputs': outputs}\n",
    "    return data\n",
    "        \n",
    "def load_entity_strings(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def get_entity_wd_id_dict(filename):\n",
    "    out = {}\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        out[line[1]] = line[0]\n",
    "    return out\n",
    "    \n",
    "\n",
    "def create_filter_dict(data) -> Dict[str, int]:\n",
    "    filter_dict = defaultdict(list)\n",
    "    for input, output in zip(data[\"inputs\"], data[\"outputs\"]):\n",
    "        filter_dict[input].append(output)\n",
    "    return filter_dict\n",
    "\n",
    "def getAllFilteringEntities(input, filter_dicts):\n",
    "    entities = []\n",
    "    splits = ['train', 'test', 'valid']\n",
    "    for s in splits:\n",
    "        entities.extend(filter_dicts[s][input])\n",
    "    return list(set(entities))\n",
    "\n",
    "def wikidata_link_from_id(id):\n",
    "    uri = 'https://www.wikidata.org/wiki/' + id\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'fbwq_half'\n",
    "entity_strings = load_entity_strings(os.path.join(\"data\", dataset_name, \"entity_strings.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ee97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_strings_set = set(entity_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51647083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6456004/6456004 [00:06<00:00, 1030149.44it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 887439.22it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 943489.01it/s]\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "splits = ['train', 'valid', 'test']\n",
    "dataset_name = 'fbwq_half'\n",
    "for split in splits:\n",
    "    data[split] = loadData(os.path.join('data', dataset_name, split + '.txt'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616e3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dicts = {}\n",
    "splits = ['train', 'valid', 'test']\n",
    "for split in splits:\n",
    "    filter_dicts[split] = create_filter_dict(data[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13a5a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'scores.pickle'\n",
    "fname = 'scores/scores_temp.pickle'\n",
    "# fname = 'scores_500_base_trie.pickle'\n",
    "scores_data = pickle.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bff88b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1639it [00:00, 281636.46it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_scores_dicts = []\n",
    "for pred, score in tqdm(zip(scores_data['prediction_strings'], scores_data['scores'])):\n",
    "    ps_pair = (pred, score)\n",
    "    # remove predictions that are not entities\n",
    "    ps_pairs = [ps_pair]\n",
    "    ps_dict_only_entities = defaultdict(list)\n",
    "    for ps in ps_pairs:\n",
    "        if True:\n",
    "#         if ps[0] in entity_strings_set:\n",
    "            ps_dict_only_entities[ps[0]] = ps[1]\n",
    "    predictions_scores_dicts.append(ps_dict_only_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21902383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Japan 1', 'Okuma']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 9\n",
    "scores_data['target_strings'][id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45377700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'Fukushima': -0.001003265380859375})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_scores_dicts[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cae07f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predict answer: Fukushima Daiichi Nuclear Powe | where is the fukushima daiichi nuclear plant located NE |'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_data['input_strings'][id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3cf0d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# no filtering for QA\n",
    "predictions_filtered = predictions_scores_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cde8c8c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f6626b2e236f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_small_arrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_strings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mps_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "count = {}\n",
    "reciprocal_ranks = 0.0\n",
    "k_list = [1,3,10]\n",
    "for k in k_list:\n",
    "    count[k] = 0\n",
    "num_small_arrs = 0\n",
    "for i in range(len(predictions_filtered)):\n",
    "    targets = scores_data['target_strings'][i]\n",
    "    ps_dict = predictions_filtered[i]\n",
    "    ps_sorted = sorted(ps_dict.items(), key=lambda item: -item[1])\n",
    "#     print(ps_dict)\n",
    "    if len(ps_dict) == 0:\n",
    "        preds = []\n",
    "    else:\n",
    "        preds = [x[0] for x in ps_sorted]\n",
    "    pred = preds[0]\n",
    "    \n",
    "    if pred in targets:\n",
    "        rank = preds.index(pred) + 1\n",
    "        reciprocal_ranks += 1./rank\n",
    "for k in k_list:\n",
    "    hits_at_k = count[k]/len(predictions_filtered)\n",
    "    print('hits@{}'.format(k), hits_at_k)\n",
    "print('mrr', reciprocal_ranks/len(predictions_filtered))\n",
    "print(num_small_arrs/len(predictions_filtered), 'were <10 length preds array without answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b079bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict answer: Justin Bieber | what are the songs that justin bieber wrote NE | Target: ['Lolly', 'Home to Mama', 'Baby 3', 'Never Let You Go', 'Eenie Meenie', 'Somebody to Love 2', 'Never Say Never 0', 'Bigger', 'Pray 1', 'First Dance 0', 'Live My Life', 'Boyfriend 1', \"Turn to You (Mother's Day Dedi\", 'Die in Your Arms', 'Thought Of You', 'Beauty And A Beat', 'All Around The World', 'As Long as You Love Me 0', 'Right Here', '#thatPower', 'Heartbreaker 10', 'All That Matters 1', 'Hold Tight 0', 'Wait for a Minute', 'Recovery 3', 'Bad Day 1', 'All Bad', 'PYD', 'Change Me 0', 'Roller Coaster', 'Confident']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('Jaxon Bieber', -1.549971580505371),\n",
       " ['Lolly',\n",
       "  'Home to Mama',\n",
       "  'Baby 3',\n",
       "  'Never Let You Go',\n",
       "  'Eenie Meenie',\n",
       "  'Somebody to Love 2',\n",
       "  'Never Say Never 0',\n",
       "  'Bigger',\n",
       "  'Pray 1',\n",
       "  'First Dance 0',\n",
       "  'Live My Life',\n",
       "  'Boyfriend 1',\n",
       "  \"Turn to You (Mother's Day Dedi\",\n",
       "  'Die in Your Arms',\n",
       "  'Thought Of You',\n",
       "  'Beauty And A Beat',\n",
       "  'All Around The World',\n",
       "  'As Long as You Love Me 0',\n",
       "  'Right Here',\n",
       "  '#thatPower',\n",
       "  'Heartbreaker 10',\n",
       "  'All That Matters 1',\n",
       "  'Hold Tight 0',\n",
       "  'Wait for a Minute',\n",
       "  'Recovery 3',\n",
       "  'Bad Day 1',\n",
       "  'All Bad',\n",
       "  'PYD',\n",
       "  'Change Me 0',\n",
       "  'Roller Coaster',\n",
       "  'Confident'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 30\n",
    "inputs = scores_data['input_strings'][id]\n",
    "preds = predictions_filtered[id]\n",
    "preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "target = scores_data['target_strings'][id]\n",
    "print(inputs, 'Target:', target)\n",
    "preds[0], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "641f80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeadEntity(input):\n",
    "    x = input.split(':')[1][1:]\n",
    "    ent = x.split('|')[0][:-1]\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f8d1443a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if answer is head entity\n",
    "count = 0\n",
    "for id in range(len(predictions_filtered)):\n",
    "    input = scores_data['input_strings'][id]\n",
    "    head = getHeadEntity(input)\n",
    "    pred = scores_data['prediction_strings'][id]\n",
    "    if head == pred:\n",
    "        count += 1\n",
    "        print(input)\n",
    "count/len(predictions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8d6fa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count', 0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only head/tails\n",
    "count = 0\n",
    "for id in range(60,120, 2):\n",
    "    inputs = scores_data['input_strings'][id]\n",
    "    preds = predictions_filtered[id]\n",
    "    preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "    target = scores_data['target_strings'][id]\n",
    "    pred1 = preds[0][0]\n",
    "    if pred1 == target:\n",
    "        print(int(id/2), inputs, pred1)\n",
    "        count += 1\n",
    "'count', count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "811dccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "print(\"<a href='your_url_here'>Showing Text</a>\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "print(\"<a href='your_url_here'>Showing Text</a>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "36ee8176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4121082'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2wdid['pakistan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8dee3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['english', 'english language', 'french']\n",
    "t = Trie(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fdb192fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6aa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
